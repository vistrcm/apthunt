{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def maybe_pickle(data, filename, force=False):\n",
    "    if os.path.exists(filename) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping pickling.' % filename)\n",
    "    else:\n",
    "        print('Pickling %s.' % filename)\n",
    "        try:\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', filename, ':', e)\n",
    "\n",
    "\n",
    "def maybe_download(table, force=False):\n",
    "    storage_file = table + \".pkl\"\n",
    "    if force or not os.path.exists(storage_file):\n",
    "        data = data_retrieve(table)\n",
    "        maybe_pickle(data, storage_file, force)\n",
    "    return storage_file\n",
    "        \n",
    "def data_retrieve(table, page_size=100):\n",
    "    \"\"\"download data from table\"\"\"\n",
    "    client = boto3.client('dynamodb')\n",
    "    paginator = client.get_paginator('scan')\n",
    "\n",
    "    counter = 0\n",
    "    items = []\n",
    "    for page in paginator.paginate(TableName=table, PaginationConfig={\"PageSize\": page_size}):\n",
    "        items.extend(page[\"Items\"])\n",
    "        \n",
    "        counter += 1\n",
    "        if counter % 50 == 0:  # print debug every 50 pages\n",
    "            print(\"items: {}. next page. {}. Count {}. ScannedCount: {}\".format(len(items), counter, page[\"Count\"], page[\"ScannedCount\"]))\n",
    "\n",
    "\n",
    "    return items\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items: 5000. next page. 50. Count 100. ScannedCount: 100\n",
      "items: 10000. next page. 100. Count 100. ScannedCount: 100\n",
      "items: 15000. next page. 150. Count 100. ScannedCount: 100\n",
      "items: 20000. next page. 200. Count 100. ScannedCount: 100\n",
      "items: 25000. next page. 250. Count 100. ScannedCount: 100\n",
      "items: 30000. next page. 300. Count 100. ScannedCount: 100\n",
      "items: 35000. next page. 350. Count 100. ScannedCount: 100\n",
      "items: 40000. next page. 400. Count 100. ScannedCount: 100\n",
      "items: 45000. next page. 450. Count 100. ScannedCount: 100\n",
      "items: 50000. next page. 500. Count 100. ScannedCount: 100\n",
      "items: 55000. next page. 550. Count 100. ScannedCount: 100\n",
      "items: 60000. next page. 600. Count 100. ScannedCount: 100\n",
      "items: 65000. next page. 650. Count 100. ScannedCount: 100\n",
      "items: 70000. next page. 700. Count 100. ScannedCount: 100\n",
      "items: 75000. next page. 750. Count 100. ScannedCount: 100\n",
      "items: 80000. next page. 800. Count 100. ScannedCount: 100\n",
      "items: 85000. next page. 850. Count 100. ScannedCount: 100\n",
      "items: 90000. next page. 900. Count 100. ScannedCount: 100\n",
      "items: 95000. next page. 950. Count 100. ScannedCount: 100\n",
      "items: 100000. next page. 1000. Count 100. ScannedCount: 100\n",
      "items: 105000. next page. 1050. Count 100. ScannedCount: 100\n",
      "items: 110000. next page. 1100. Count 100. ScannedCount: 100\n",
      "items: 115000. next page. 1150. Count 100. ScannedCount: 100\n",
      "items: 120000. next page. 1200. Count 100. ScannedCount: 100\n",
      "items: 125000. next page. 1250. Count 100. ScannedCount: 100\n",
      "items: 130000. next page. 1300. Count 100. ScannedCount: 100\n",
      "items: 135000. next page. 1350. Count 100. ScannedCount: 100\n",
      "items: 140000. next page. 1400. Count 100. ScannedCount: 100\n",
      "items: 145000. next page. 1450. Count 100. ScannedCount: 100\n",
      "items: 150000. next page. 1500. Count 100. ScannedCount: 100\n",
      "items: 155000. next page. 1550. Count 100. ScannedCount: 100\n",
      "items: 160000. next page. 1600. Count 100. ScannedCount: 100\n",
      "items: 165000. next page. 1650. Count 100. ScannedCount: 100\n",
      "items: 170000. next page. 1700. Count 100. ScannedCount: 100\n",
      "items: 175000. next page. 1750. Count 100. ScannedCount: 100\n",
      "items: 180000. next page. 1800. Count 100. ScannedCount: 100\n",
      "items: 185000. next page. 1850. Count 100. ScannedCount: 100\n",
      "items: 190000. next page. 1900. Count 100. ScannedCount: 100\n",
      "items: 195000. next page. 1950. Count 100. ScannedCount: 100\n",
      "items: 200000. next page. 2000. Count 100. ScannedCount: 100\n",
      "items: 205000. next page. 2050. Count 100. ScannedCount: 100\n",
      "items: 210000. next page. 2100. Count 100. ScannedCount: 100\n",
      "items: 215000. next page. 2150. Count 100. ScannedCount: 100\n",
      "items: 220000. next page. 2200. Count 100. ScannedCount: 100\n",
      "items: 225000. next page. 2250. Count 100. ScannedCount: 100\n",
      "items: 230000. next page. 2300. Count 100. ScannedCount: 100\n",
      "items: 235000. next page. 2350. Count 100. ScannedCount: 100\n",
      "items: 240000. next page. 2400. Count 100. ScannedCount: 100\n",
      "items: 245000. next page. 2450. Count 100. ScannedCount: 100\n",
      "items: 250000. next page. 2500. Count 100. ScannedCount: 100\n",
      "items: 255000. next page. 2550. Count 100. ScannedCount: 100\n",
      "items: 260000. next page. 2600. Count 100. ScannedCount: 100\n",
      "items: 265000. next page. 2650. Count 100. ScannedCount: 100\n",
      "items: 270000. next page. 2700. Count 100. ScannedCount: 100\n",
      "items: 275000. next page. 2750. Count 100. ScannedCount: 100\n",
      "items: 280000. next page. 2800. Count 100. ScannedCount: 100\n",
      "items: 285000. next page. 2850. Count 100. ScannedCount: 100\n",
      "items: 290000. next page. 2900. Count 100. ScannedCount: 100\n",
      "items: 295000. next page. 2950. Count 100. ScannedCount: 100\n",
      "items: 300000. next page. 3000. Count 100. ScannedCount: 100\n",
      "items: 305000. next page. 3050. Count 100. ScannedCount: 100\n",
      "items: 310000. next page. 3100. Count 100. ScannedCount: 100\n",
      "items: 315000. next page. 3150. Count 100. ScannedCount: 100\n",
      "items: 320000. next page. 3200. Count 100. ScannedCount: 100\n",
      "items: 325000. next page. 3250. Count 100. ScannedCount: 100\n",
      "items: 330000. next page. 3300. Count 100. ScannedCount: 100\n",
      "items: 335000. next page. 3350. Count 100. ScannedCount: 100\n",
      "items: 340000. next page. 3400. Count 100. ScannedCount: 100\n",
      "items: 345000. next page. 3450. Count 100. ScannedCount: 100\n",
      "items: 350000. next page. 3500. Count 100. ScannedCount: 100\n",
      "items: 355000. next page. 3550. Count 100. ScannedCount: 100\n",
      "items: 360000. next page. 3600. Count 100. ScannedCount: 100\n",
      "items: 365000. next page. 3650. Count 100. ScannedCount: 100\n",
      "items: 370000. next page. 3700. Count 100. ScannedCount: 100\n",
      "items: 375000. next page. 3750. Count 100. ScannedCount: 100\n",
      "items: 380000. next page. 3800. Count 100. ScannedCount: 100\n",
      "items: 385000. next page. 3850. Count 100. ScannedCount: 100\n",
      "items: 390000. next page. 3900. Count 100. ScannedCount: 100\n",
      "items: 395000. next page. 3950. Count 100. ScannedCount: 100\n",
      "items: 400000. next page. 4000. Count 100. ScannedCount: 100\n",
      "items: 405000. next page. 4050. Count 100. ScannedCount: 100\n",
      "items: 410000. next page. 4100. Count 100. ScannedCount: 100\n",
      "items: 415000. next page. 4150. Count 100. ScannedCount: 100\n",
      "items: 420000. next page. 4200. Count 100. ScannedCount: 100\n",
      "items: 425000. next page. 4250. Count 100. ScannedCount: 100\n",
      "items: 430000. next page. 4300. Count 100. ScannedCount: 100\n",
      "items: 435000. next page. 4350. Count 100. ScannedCount: 100\n",
      "items: 440000. next page. 4400. Count 100. ScannedCount: 100\n",
      "items: 445000. next page. 4450. Count 100. ScannedCount: 100\n",
      "items: 450000. next page. 4500. Count 100. ScannedCount: 100\n",
      "items: 455000. next page. 4550. Count 100. ScannedCount: 100\n",
      "items: 460000. next page. 4600. Count 100. ScannedCount: 100\n",
      "items: 465000. next page. 4650. Count 100. ScannedCount: 100\n",
      "items: 470000. next page. 4700. Count 100. ScannedCount: 100\n",
      "items: 475000. next page. 4750. Count 100. ScannedCount: 100\n",
      "items: 480000. next page. 4800. Count 100. ScannedCount: 100\n",
      "items: 485000. next page. 4850. Count 100. ScannedCount: 100\n",
      "items: 490000. next page. 4900. Count 100. ScannedCount: 100\n",
      "items: 495000. next page. 4950. Count 100. ScannedCount: 100\n",
      "items: 500000. next page. 5000. Count 100. ScannedCount: 100\n",
      "items: 505000. next page. 5050. Count 100. ScannedCount: 100\n",
      "items: 510000. next page. 5100. Count 100. ScannedCount: 100\n",
      "items: 515000. next page. 5150. Count 100. ScannedCount: 100\n",
      "items: 520000. next page. 5200. Count 100. ScannedCount: 100\n",
      "items: 525000. next page. 5250. Count 100. ScannedCount: 100\n",
      "items: 530000. next page. 5300. Count 100. ScannedCount: 100\n",
      "items: 535000. next page. 5350. Count 100. ScannedCount: 100\n",
      "items: 540000. next page. 5400. Count 100. ScannedCount: 100\n",
      "items: 545000. next page. 5450. Count 100. ScannedCount: 100\n",
      "items: 550000. next page. 5500. Count 100. ScannedCount: 100\n",
      "items: 555000. next page. 5550. Count 100. ScannedCount: 100\n",
      "items: 560000. next page. 5600. Count 100. ScannedCount: 100\n",
      "items: 565000. next page. 5650. Count 100. ScannedCount: 100\n",
      "items: 570000. next page. 5700. Count 100. ScannedCount: 100\n",
      "items: 575000. next page. 5750. Count 100. ScannedCount: 100\n",
      "items: 580000. next page. 5800. Count 100. ScannedCount: 100\n",
      "items: 585000. next page. 5850. Count 100. ScannedCount: 100\n",
      "items: 590000. next page. 5900. Count 100. ScannedCount: 100\n",
      "items: 595000. next page. 5950. Count 100. ScannedCount: 100\n",
      "items: 600000. next page. 6000. Count 100. ScannedCount: 100\n",
      "items: 605000. next page. 6050. Count 100. ScannedCount: 100\n",
      "items: 610000. next page. 6100. Count 100. ScannedCount: 100\n",
      "items: 615000. next page. 6150. Count 100. ScannedCount: 100\n",
      "items: 620000. next page. 6200. Count 100. ScannedCount: 100\n",
      "items: 625000. next page. 6250. Count 100. ScannedCount: 100\n",
      "items: 630000. next page. 6300. Count 100. ScannedCount: 100\n",
      "items: 635000. next page. 6350. Count 100. ScannedCount: 100\n",
      "items: 640000. next page. 6400. Count 100. ScannedCount: 100\n",
      "items: 645000. next page. 6450. Count 100. ScannedCount: 100\n",
      "items: 650000. next page. 6500. Count 100. ScannedCount: 100\n",
      "items: 655000. next page. 6550. Count 100. ScannedCount: 100\n",
      "items: 660000. next page. 6600. Count 100. ScannedCount: 100\n",
      "items: 665000. next page. 6650. Count 100. ScannedCount: 100\n",
      "items: 670000. next page. 6700. Count 100. ScannedCount: 100\n",
      "items: 675000. next page. 6750. Count 100. ScannedCount: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items: 680000. next page. 6800. Count 100. ScannedCount: 100\n",
      "items: 685000. next page. 6850. Count 100. ScannedCount: 100\n",
      "items: 690000. next page. 6900. Count 100. ScannedCount: 100\n",
      "items: 695000. next page. 6950. Count 100. ScannedCount: 100\n",
      "items: 700000. next page. 7000. Count 100. ScannedCount: 100\n",
      "items: 705000. next page. 7050. Count 100. ScannedCount: 100\n",
      "items: 710000. next page. 7100. Count 100. ScannedCount: 100\n",
      "items: 715000. next page. 7150. Count 100. ScannedCount: 100\n",
      "items: 720000. next page. 7200. Count 100. ScannedCount: 100\n",
      "items: 725000. next page. 7250. Count 100. ScannedCount: 100\n",
      "items: 730000. next page. 7300. Count 100. ScannedCount: 100\n",
      "items: 735000. next page. 7350. Count 100. ScannedCount: 100\n",
      "items: 740000. next page. 7400. Count 100. ScannedCount: 100\n",
      "items: 745000. next page. 7450. Count 100. ScannedCount: 100\n",
      "items: 750000. next page. 7500. Count 100. ScannedCount: 100\n",
      "items: 755000. next page. 7550. Count 100. ScannedCount: 100\n",
      "items: 760000. next page. 7600. Count 100. ScannedCount: 100\n",
      "items: 765000. next page. 7650. Count 100. ScannedCount: 100\n",
      "items: 770000. next page. 7700. Count 100. ScannedCount: 100\n",
      "items: 775000. next page. 7750. Count 100. ScannedCount: 100\n",
      "items: 780000. next page. 7800. Count 100. ScannedCount: 100\n",
      "items: 785000. next page. 7850. Count 100. ScannedCount: 100\n",
      "items: 790000. next page. 7900. Count 100. ScannedCount: 100\n",
      "items: 795000. next page. 7950. Count 100. ScannedCount: 100\n",
      "items: 800000. next page. 8000. Count 100. ScannedCount: 100\n",
      "items: 805000. next page. 8050. Count 100. ScannedCount: 100\n",
      "items: 810000. next page. 8100. Count 100. ScannedCount: 100\n",
      "items: 815000. next page. 8150. Count 100. ScannedCount: 100\n",
      "items: 820000. next page. 8200. Count 100. ScannedCount: 100\n",
      "items: 825000. next page. 8250. Count 100. ScannedCount: 100\n",
      "items: 830000. next page. 8300. Count 100. ScannedCount: 100\n",
      "items: 835000. next page. 8350. Count 100. ScannedCount: 100\n",
      "items: 840000. next page. 8400. Count 100. ScannedCount: 100\n",
      "items: 845000. next page. 8450. Count 100. ScannedCount: 100\n",
      "items: 850000. next page. 8500. Count 100. ScannedCount: 100\n",
      "items: 855000. next page. 8550. Count 100. ScannedCount: 100\n",
      "items: 860000. next page. 8600. Count 100. ScannedCount: 100\n",
      "items: 865000. next page. 8650. Count 100. ScannedCount: 100\n",
      "items: 870000. next page. 8700. Count 100. ScannedCount: 100\n",
      "items: 875000. next page. 8750. Count 100. ScannedCount: 100\n",
      "items: 880000. next page. 8800. Count 100. ScannedCount: 100\n",
      "items: 885000. next page. 8850. Count 100. ScannedCount: 100\n",
      "items: 890000. next page. 8900. Count 100. ScannedCount: 100\n",
      "items: 895000. next page. 8950. Count 100. ScannedCount: 100\n",
      "items: 900000. next page. 9000. Count 100. ScannedCount: 100\n",
      "items: 905000. next page. 9050. Count 100. ScannedCount: 100\n",
      "items: 910000. next page. 9100. Count 100. ScannedCount: 100\n",
      "items: 915000. next page. 9150. Count 100. ScannedCount: 100\n",
      "items: 920000. next page. 9200. Count 100. ScannedCount: 100\n",
      "items: 925000. next page. 9250. Count 100. ScannedCount: 100\n",
      "items: 930000. next page. 9300. Count 100. ScannedCount: 100\n",
      "items: 935000. next page. 9350. Count 100. ScannedCount: 100\n",
      "items: 940000. next page. 9400. Count 100. ScannedCount: 100\n",
      "items: 945000. next page. 9450. Count 100. ScannedCount: 100\n",
      "items: 950000. next page. 9500. Count 100. ScannedCount: 100\n",
      "items: 955000. next page. 9550. Count 100. ScannedCount: 100\n",
      "items: 960000. next page. 9600. Count 100. ScannedCount: 100\n",
      "items: 965000. next page. 9650. Count 100. ScannedCount: 100\n",
      "items: 970000. next page. 9700. Count 100. ScannedCount: 100\n",
      "items: 975000. next page. 9750. Count 100. ScannedCount: 100\n",
      "items: 980000. next page. 9800. Count 100. ScannedCount: 100\n",
      "items: 985000. next page. 9850. Count 100. ScannedCount: 100\n",
      "items: 990000. next page. 9900. Count 100. ScannedCount: 100\n",
      "items: 995000. next page. 9950. Count 100. ScannedCount: 100\n",
      "items: 1000000. next page. 10000. Count 100. ScannedCount: 100\n",
      "items: 1005000. next page. 10050. Count 100. ScannedCount: 100\n",
      "items: 1010000. next page. 10100. Count 100. ScannedCount: 100\n",
      "items: 1015000. next page. 10150. Count 100. ScannedCount: 100\n",
      "items: 1020000. next page. 10200. Count 100. ScannedCount: 100\n",
      "items: 1025000. next page. 10250. Count 100. ScannedCount: 100\n",
      "items: 1030000. next page. 10300. Count 100. ScannedCount: 100\n",
      "items: 1035000. next page. 10350. Count 100. ScannedCount: 100\n",
      "items: 1040000. next page. 10400. Count 100. ScannedCount: 100\n",
      "items: 1045000. next page. 10450. Count 100. ScannedCount: 100\n",
      "items: 1050000. next page. 10500. Count 100. ScannedCount: 100\n",
      "items: 1055000. next page. 10550. Count 100. ScannedCount: 100\n",
      "items: 1060000. next page. 10600. Count 100. ScannedCount: 100\n",
      "items: 1065000. next page. 10650. Count 100. ScannedCount: 100\n",
      "items: 1070000. next page. 10700. Count 100. ScannedCount: 100\n",
      "items: 1075000. next page. 10750. Count 100. ScannedCount: 100\n",
      "items: 1080000. next page. 10800. Count 100. ScannedCount: 100\n",
      "items: 1085000. next page. 10850. Count 100. ScannedCount: 100\n",
      "items: 1090000. next page. 10900. Count 100. ScannedCount: 100\n",
      "items: 1095000. next page. 10950. Count 100. ScannedCount: 100\n",
      "items: 1100000. next page. 11000. Count 100. ScannedCount: 100\n",
      "items: 1105000. next page. 11050. Count 100. ScannedCount: 100\n",
      "items: 1110000. next page. 11100. Count 100. ScannedCount: 100\n",
      "items: 1115000. next page. 11150. Count 100. ScannedCount: 100\n",
      "items: 1120000. next page. 11200. Count 100. ScannedCount: 100\n",
      "items: 1125000. next page. 11250. Count 100. ScannedCount: 100\n",
      "Pickling apthunt.pkl.\n"
     ]
    }
   ],
   "source": [
    "# data_file = maybe_download(\"apthunt\")\n",
    "data_file = maybe_download(\"apthunt\", force=True)\n",
    "data = pickle.load(open(data_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow\n"
     ]
    }
   ],
   "source": [
    "print(\"wow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# To go from dynamo format to python\n",
    "from boto3.dynamodb import types\n",
    "\n",
    "deserializer = types.TypeDeserializer()\n",
    "python_data = [{k: deserializer.deserialize(v) for k,v in d.items()} for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(python_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del python_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"df.pkl\", 'wb') as f:\n",
    "    pickle.dump(df, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## can restore from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open(\"df.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small cleanup of \"simulated\"\n",
    "del df[\"FeedTitle\"]\n",
    "del df[\"FeedUrl\"]\n",
    "del df[\"PostContent\"]\n",
    "del df[\"PostPublished\"]\n",
    "del df[\"PostTitle\"]\n",
    "# delete parsed_notices. looks like it is the same\n",
    "del df[\"parsed_notices\"]\n",
    "# no need for price text\n",
    "del df[\"parsed_price_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parsed_args(args):\n",
    "    if not isinstance(args, list):\n",
    "        return []\n",
    "    clean = []\n",
    "    for p in args:\n",
    "        # remove days\n",
    "        if p.startswith(\"friday\") or p.startswith(\"monday\") or p.startswith(\"saturday\") or p.startswith(\"sunday\") or p.startswith(\"thursday\") or p.startswith(\"tuesday\") or p.startswith(\"wednesday\"):\n",
    "            continue\n",
    "        # remove br/ba\n",
    "        if \"BR / \" in p: continue\n",
    "        # remove ft2\n",
    "        if p.endswith(\"ft2\"): continue\n",
    "\n",
    "        # remove application fee\n",
    "        if p.startswith(\"application fee\"): continue\n",
    "        \n",
    "        # remove broker fee\n",
    "        if p.startswith(\"broker fee\"): continue\n",
    "        \n",
    "        # remove available\n",
    "        if p.startswith(\"available\"): continue\n",
    "        \n",
    "        # remove listed by\n",
    "        if p.startswith(\"listed by\"): continue\n",
    "        clean.append(p)\n",
    "    return clean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_attrs'] = df['parsed_attrs'].map(clean_parsed_args)\n",
    "# one_hot = pd.get_dummies(df['parsed_attrs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_attrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(attrs):\n",
    "    if not isinstance(attrs, list):\n",
    "        return None\n",
    "    types = set([\n",
    "        \"apartment\", \n",
    "        \"townhouse\", \n",
    "        \"loft\", \n",
    "        \"land\", \n",
    "        \"house\", \n",
    "        \"duplex\", \n",
    "        \"flat\", \n",
    "        \"condo\", \n",
    "        \"cottage/cabin\"\n",
    "    ])\n",
    "    return \",\".join(sorted(types & set(attrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    new  = pd.DataFrame()\n",
    "    new[\"PostUrl\"] = df[\"PostUrl\"]\n",
    "    new[\"latitude\"] = df[\"parsed_data_latitude\"]\n",
    "    new[\"longitude\"] = df[\"parsed_data_longitude\"]\n",
    "    new[\"district\"] = df[\"parsed_district\"].map(lambda s: s.strip(\"()\") if isinstance(s, str) else s)\n",
    "    new[\"address\"] = df[\"parsed_map_address\"].map(lambda x: x if x != \"(google map)\" else None)\n",
    "    new[\"housing\"] = df[\"parsed_housing\"]\n",
    "    \n",
    "    new[\"price\"] = df[\"parsed_price\"].map(lambda x: float(x) if x else x)\n",
    "    \n",
    "    # to numerical values\n",
    "    new['price'] = pd.to_numeric(new['price'])\n",
    "    new['longitude'] = pd.to_numeric(new['longitude'])\n",
    "    new['latitude'] = pd.to_numeric(new['latitude'])\n",
    "    \n",
    "    new['bedrooms'] = new['housing'].str.extract('(\\d+)br\\s.*', expand=True)\n",
    "    new['bedrooms'] = pd.to_numeric(new['bedrooms'])\n",
    "    \n",
    "    new['area'] = new['housing'].str.extract('(\\d+)ft2', expand=True)\n",
    "    new['area'] = pd.to_numeric(new['area'])\n",
    "    \n",
    "    new['type'] = df[\"parsed_attrs\"].map(get_type)\n",
    "    new['catsok'] = df[\"parsed_attrs\"].map(lambda x: \"cats are OK - purrr\" in x)\n",
    "    new['dogsok'] = df[\"parsed_attrs\"].map(lambda x: \"dogs are OK - wooof\" in x)\n",
    "    new['garagea'] = df[\"parsed_attrs\"].map(lambda x: \"attached garage\" in x)\n",
    "    new['garaged'] = df[\"parsed_attrs\"].map(lambda x: \"detached garage\" in x)\n",
    "    new['furnished'] = df[\"parsed_attrs\"].map(lambda x: \"furnished\" in x)\n",
    "    \n",
    "    new['laundryb'] = df[\"parsed_attrs\"].map(lambda x: \"laundry in bldg\" in x)\n",
    "    new['laundrys'] = df[\"parsed_attrs\"].map(lambda x: \"laundry on site\" in x)\n",
    "    \n",
    "    new['wd'] = df[\"parsed_attrs\"].map(lambda x: \"w/d in unit\" in x)\n",
    "    new['nthumbs'] = df[\"parsed_thumbs\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short.dropna(subset=[\"price\"], inplace=True)\n",
    "short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short[\"price\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all prices more than 9k to to 9 k\n",
    "short.loc[short['price'] > 9000, \"price\"] = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short[\"price\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short.drop_duplicates(inplace=True)\n",
    "short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short['price_bucket'] = short['price'].apply(lambda x: f\"{x//500:02.0f}x500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short[\"price_bucket\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = Path(\"./apthuntdata\")\n",
    "export_path.mkdir(exist_ok=True)\n",
    "short.to_csv(export_path/\"data.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short[\"price\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(short.loc[:, \"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(short.loc[short[\"price\"] < 20000, \"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short[short.price > 15000].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1094/578376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_diff(inp, target):\n",
    "    return (inp - target).abs().mean()\n",
    "def min_abs_diff(inp, target):\n",
    "    return (inp - target).abs().min()\n",
    "def max_abs_diff(inp, target):\n",
    "    return (inp - target).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_inf = load_learner('cltab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = pd.read_csv(\"apthuntdata/data.csv\").loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tp[\"PostUrl\"]\n",
    "del tp[\"address\"]\n",
    "del tp[\"price\"]\n",
    "del tp[\"price_bucket\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = learn_inf.predict(tp)\n",
    "i = 0\n",
    "for p in prediction:\n",
    "    print(\"-\" * 11)\n",
    "    print(i)\n",
    "    i += 1\n",
    "    print(p)\n",
    "    print(type(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"apthuntdata/data.csv\").loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topred = pd.read_csv(\"apthuntdata/data.csv\")\n",
    "del topred[\"PostUrl\"]\n",
    "del topred[\"address\"]\n",
    "del topred[\"price\"]\n",
    "del topred[\"price_bucket\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_inf.predict(topred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topred.loc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topred.apply(lambda x: f\"{x['latitude']} + {x['district']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topred[\"price\"] = topred.apply(learn_inf.predict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
